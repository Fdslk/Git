---
title: 文本表示
grammar_cjkRuby: true
---
*文本表示和特征选择*
1.空间向量模型![enter description here][1]
特征：指出现在文档啊中并能表示该文档特点的基本语言单位（如英文等西方语言中的单词，汉语、日语等语言中的字和词、词组）；	
特征项权重：指特征项能够代表文档D能力的大小，它体现了该特征项在文档D中的重要程度。
![enter description here][2]
表示为文档向量的线性表的存储方式和链表的存储方式
![enter description here][3]
使用链表存储文本向量，节约空间，因为在某个维度下可能有多个的0的值
相似度S的计算：
曼哈顿距离、欧几里得距离、向量内积、余弦相似度。后面两者就是两者的夹角越小相似度越高。
![enter description here][4]w表示每个特征项的权重，n表示总数，D表示不同文档

文本预处理：1.字符编码转换2.中文文本分词（最重要）3.停用词过滤（停用词表中主要有数字，西方语言中的字母，还有大量的中英日俄等语言的常用词，这些词一般都是虚词、量词等，他们几乎出现在所有类别中的文档中，因而不具有任何类别信息，不能作为特征使用。）

特征权值计算：
![enter description here][5]
IDF inverse Document Frequency（逆文档频率）

文本间的相似度计算：.
使用余弦相似度来计算待分类文档和训练集中文档间的相似度值
![enter description here][6]

特征选择算法：文档频率（Document Frequency）、互信息（Mutual Information）、信息增益（Information Gain）、X^2统计量
1.提高分类效率、2.是文本之间的相似度更加准确，既提高语义上相关文本之间的相似度同时降低语义上不相关文本之间的相似度3.提高分类器的推广能力

*文本分类算法*
*1.基于统计的的方法：*
简单贝叶斯，K最近邻方法（KNN），类中心向量，回归模型，支持向量机（SVM），最大熵模型
*2基于连接的方法：* 人工神经网络
*3.基于规则的发方法：* 决策树

主要使用SVM， 和KNN
SVM该方法是从可分情况下的最优分类面提出的![enter description here][7]
过两类样本中离分类面最近的店且平行于最优分类面的超平面H1，H2上的训练样本就是上式中使等号成立的那些样本，它们叫做支持向量，因为它们职称了最优分类面。
对于非线性问题，输入空间可以通过非线性变换化为某个高维空间中的线性问题，在变换空间求最优分类面。这种变换可能比较复杂,特别是随着输入空间样本数的增大,特征空间的维数很快变得不可计算,这时就需要核函数将特征映射到高维空间定义非线性映射中“令将输入空间的样本映射到高维特征空间中,当在中构造最优超平面时,训练算法仅使用空间中的内积。
![enter description here][8]


KNN算法：
![enter description here][9]
当出现了两都是邻近的最大值得数值时，需要使用相似度辅助KNN分类，当有相同的时候就取相似度最大的那个类最为文档最后被判为的类。


文本分类过程：建立数据集，文本预处理，特征选择，建立分类模型，测试和分类质量评价
具体步骤：
![enter description here][10]

*基于划分的聚类算法：*
1.K-平均（K-means）算法：一种最简单的无监督学习算法，它是通过一种简单以实现的方法将n个对象分到k个类别。将k个类别设置为质心，然后把数据集中的数据对象分到离他们最近的质心k，这就完成了一次聚类；然后在重新生成新的k个质心，再分类，最后分类到聚类的过程不再变化。
![enter description here][11]
2.K-中心点（K-medoids）算法：每种类中心点需要的满足的条件，就是这个点到这个类中其它所有点的距离和为最小值（所谓距离就是训练集与数据集对象的相似度），使用该种算法可以有效的避免中心点为平均值容易受大数据的影响的问题

对于聚类形成的类，我们如何判断是否为最好的一个聚类呢：
![enter description here][12]

*VC 维*
vc维中的重要函数 增长函数，对分函数，打散函数，断点函数
1.增长函数
	表示假设空间H对m个示例所能赋予标记的最可能结果数。函数增长值越大则假设空间H得表示能力越强，复杂度也越高，学习啊任务的适应能力越强。对于有m个示例的数据集，最多只能有2^m个标记的结果，但大多数情况下达不到这个值。
![enter description here][13]
其实说白就是对一个情况的预期的结果假设，将所有的预期结果全部罗列出来。
2.对分函数
	对于二分类问题来说，H中的假设对D中m个示例赋予标记的每种可能结果成为对D的一种对分。对分也是增长函数的一种上限。m个示例，可能得到结果就会有2^m种可能，这是采用二分法来进行假设的。
3.打散函数
	打散指的是假设空间H能实现数据集D上全部示例的对分，即增长函数=2^m。但是认识到不打散更加重要。
	有些情况下H的增长函数不可以达到对应的2^m值，比如说在二维平面上的线性划分情况中，以下的情况就不可线性可分(也就是说不能算作赋予标记的结果)：
	打散的话就是说在有M种的样本中，需要有2^M种可能才能叫做被打散，打散就是每一种可能都要能够列举出来。
4.break point（不要理解为断点，应该理解为破点）

*SVM 算法*
通过给定的训练集合，寻找一个确定的超平面，该超平面可以将整个训练集合中的数据相互区别开来，并且与类别边界的沿纯质与该超平面方向的距离最大，所以SVM分类算法也被称为最大边缘算法。对于一般的线性问题，SVM方法很好解决，但是对于一些非线性文本分类情况，就得利用支持向量机中给定的核函数将非线性样本映射到高维的线性空间中解决。对于文本分类来说，目前支持向向量机方法时分类效果中最好的一种方法。
实现方法：需要根据实际的分类问题确定支持向量机，之歌过程需要解决两个问题：第一，找到一个非线性函数，从而把那些非线性样本数据映射到高维的线性空间，第二，在映射的高维空间中，如何确定最优超平面。

SVM解决非线性可分问题的思路：通过非线性变换函数![enter description here][14]，降低为输入的非线性样本变量空间转换到高维的线性空间中，其中引入的非线性变换函数![enter description here][15]，叫做支持向量机核函数。对于寻优函数和分类函数，他们的计算均只涉及到训练集合内部样本之间的内积运算，因此，转换后的高维空间也只需要进行内积运算，即核函数![enter description here][16]的运算。

提高SVM分类模型性能，最重要的就是要选择合适的核函数，主要的探讨的部分是：
1.核函数自身性质的研究。
2.SVM中核函数的构建搞糟方法。
3.核函数中乙烯类参数的优化性能选择。

支持向量机核函数的线性核函数
1.线性核函数
2.多项式核函数
3.径向式核函数
4.Sigmoid核函数

### *基于文档频率（DF）特征选择方法*
 1.对特征词列表wordlist初始化，置为NULL
 2.遍历文档集C,逐个读取文档D,生成原始wordlist
 3.再次遍历文档集和特征词列表，计算各个特征词的DF
 foreach t in wordlist
 foreach D in C
 if t在D中出现then DF，++；
 DF（t）=DFt/N;
 其中DFt为特征词t在文档集C中出现的次数，N为文档集C中文档总数。
 4.通过多次试验来设置阀值 Dmin，Dmax
 if DF（t）小于Dmax&&大于Dmin then reservet
 else delete t from wordlist
 5.把关键词表wordlist中的关键词进行排序构成最终特征向量列表。

### 信息熵：
度量对象的混乱程度和不确定性，定义为：设X是随机变量，Xi的取值的概率为Pi=P{X=xi}i=1,2.....，n则X的熵为![enter description here][17]，文本中取u=2，当每个值出现的概率相等，即当P1=P2=.........=Pn时，![enter description here][18]。

特征的熵表示的其不确定性，可行对特征词的有效性权值重度量，因此用下面公式计算特征词的确定性：
![enter description here][19]

*改进后的计算过程描述：*
step1：对特征值列表wordlist进行初始化置空；
step2：遍历文档集C,生成原始特征集wordlist；
step3：再次遍历文档集和特征测列表，生成Pij矩阵并规格化。对于特征集wordlist中的每个特征值利用公式计算Wi和DF
![enter description here][20] ![enter description here][21]
step4：对特征值按新的权值进行排序输出。

信息熵实质上是计算信息量的期望值：
![enter description here][22]

TF-IDF 余弦文本分类
http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html

instances中的getclassIndex方法的含义
![enter description here][23]

![enter description here][24]
trainSamplePercent：这是一个比例

*KNN*
	该方法属于监督学习中的类方法的一中，如果是监督学习，就会对先训练数据有标注类别，否则为非监督学习。监督学习是根据输入数据（训练数据）学习一个模型，能对后来的输入做预测。在监督学习中，输入变量和输出变量可以是连续的，也可以是离散的。若输入变量与输出变量均为连续变量，则称为回归；输出变量为有限个离散变量，则称为分类；输入变量与输出变量均为变量序列。
	
![enter description here][25]

KNN学习模型，在分类时会使用分类损失函数，该分类损失函数为(0-1)函数分类正确为0，错误为1




  [1]: ./images/1486354227897.jpg "1486354227897.jpg"
  [2]: ./images/1486354891900.jpg "1486354891900.jpg"
  [3]: ./images/1486354917423.jpg "1486354917423.jpg"
  [4]: ./images/1486355409422.jpg "1486355409422.jpg"
  [5]: ./images/1486357447663.jpg "1486357447663.jpg"
  [6]: ./images/1486358118194.jpg "1486358118194.jpg"
  [7]: ./images/1486440002047.jpg "1486440002047.jpg"
  [8]: ./images/1486444006934.jpg "1486444006934.jpg"
  [9]: ./images/1486445233263.jpg "1486445233263.jpg"
  [10]: ./images/1486609816578.jpg "1486609816578.jpg"
  [11]: ./images/1486610695504.jpg "1486610695504.jpg"
  [12]: ./images/1486611426391.jpg "1486611426391.jpg"
  [13]: ./images/1490415575915.jpg "1490415575915.jpg"
  [14]: ./images/1490523353257.jpg "1490523353257.jpg"
  [15]: ./images/1490523416316.jpg "1490523416316.jpg"
  [16]: ./images/1490523543727.jpg "1490523543727.jpg"
  [17]: ./images/1490585146229.jpg "1490585146229.jpg"
  [18]: ./images/1490585209027.jpg "1490585209027.jpg"
  [19]: ./images/1490585532903.jpg "1490585532903.jpg"
  [20]: ./images/1491465941524.jpg "1491465941524.jpg"
  [21]: ./images/1491465961379.jpg "1491465961379.jpg"
  [22]: ./images/1491535656358.jpg "1491535656358.jpg"
  [23]: ./images/1492310329389.jpg "1492310329389.jpg"
  [24]: ./images/1492315869977.jpg "1492315869977.jpg"
  [25]: ./images/1492399660829.jpg "1492399660829.jpg"